{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13918275,"sourceType":"datasetVersion","datasetId":8868701},{"sourceId":13918483,"sourceType":"datasetVersion","datasetId":8868859},{"sourceId":13919115,"sourceType":"datasetVersion","datasetId":8869342}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:55:16.388555Z","iopub.execute_input":"2025-11-29T11:55:16.388779Z","iopub.status.idle":"2025-11-29T11:55:17.855613Z","shell.execute_reply.started":"2025-11-29T11:55:16.388753Z","shell.execute_reply":"2025-11-29T11:55:17.854808Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/hcl-tech-2/telecom_tickets_cleaned1.csv\n/kaggle/input/final-one/telecom_tickets_cleaned1.csv\n/kaggle/input/final-hackathon-code/telecom_support_dataset_expanded.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install llama-cpp-python\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom llama_cpp import Llama\n\n# 1ï¸âƒ£ Download the GGUF model\nmodel_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n\nprint(\"Downloading model...\")\n!wget $model_url -O mistral.gguf\n\n# Check file exists\nassert os.path.exists(\"mistral.gguf\"), \"Download failed!\"\n\nprint(\"Model downloaded successfully!\")\n\n# 2ï¸âƒ£ Load model\nllm = Llama(\n    model_path=\"mistral.gguf\",   # must exist\n    n_ctx=4096,\n    n_threads=4,\n    n_gpu_layers=0,\n    verbose=False\n)\n\n# 3ï¸âƒ£ Test inference\nresponse = llm(\"Write a 2 line greeting message.\")\nprint(\"\\nMODEL OUTPUT:\\n\", response[\"choices\"][0][\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:32:33.257803Z","iopub.execute_input":"2025-11-29T10:32:33.258300Z","iopub.status.idle":"2025-11-29T10:33:04.875584Z","shell.execute_reply.started":"2025-11-29T10:32:33.258260Z","shell.execute_reply":"2025-11-29T10:33:04.873970Z"}},"outputs":[{"name":"stdout","text":"Downloading model...\n--2025-11-29 10:32:33--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\nResolving huggingface.co (huggingface.co)... 3.165.160.11, 3.165.160.61, 3.165.160.59, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.11|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cas-bridge.xethub.hf.co/xet-bridge-us/65146b42c75a3d4c44e41667/a246c48a2d5d13ceb201c3e669b0b4144992648f42a1bea4c5f15832127c831f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251129%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251129T103233Z&X-Amz-Expires=3600&X-Amz-Signature=0c8dc99a3b850c06de37a03fc12d9124526b7ef7fd744efab934c6689c1e71c9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1764415953&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NDQxNTk1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTE0NmI0MmM3NWEzZDRjNDRlNDE2NjcvYTI0NmM0OGEyZDVkMTNjZWIyMDFjM2U2NjliMGI0MTQ0OTkyNjQ4ZjQyYTFiZWE0YzVmMTU4MzIxMjdjODMxZioifV19&Signature=ltxN%7EoFENPpTcM0z4FQmlEOp5bANZbrJiKEinpY-Tb8xIyn%7EERo1mLwU4gwAd2LOQzn9lrNp6XGWT2pWYNDnR1%7EuFDCNcyoDullTDyv0Fboadum8QNRq9R4sDuzNF5LEKYFVUos-t4Pemy0koqxykE0Dhc0kPXbrwIprT6dL44npXwBUO2fV6cvywo0T5NKvc11Q4T7DOAdIJAMG-uEEiJqzqYkQKQC%7EbIoyYdrM%7EWDs4gsbyHKAxFd6kqqko2gWy%7EOR%7E%7ELpc9pOdQwzQc1064SsJ8I7rF41H5P7SMeXP7ILgw1nyQ079KqfXcoxvtj8P8DZOIVo0pv7vJV25HowpA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n--2025-11-29 10:32:33--  https://cas-bridge.xethub.hf.co/xet-bridge-us/65146b42c75a3d4c44e41667/a246c48a2d5d13ceb201c3e669b0b4144992648f42a1bea4c5f15832127c831f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251129%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251129T103233Z&X-Amz-Expires=3600&X-Amz-Signature=0c8dc99a3b850c06de37a03fc12d9124526b7ef7fd744efab934c6689c1e71c9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.1.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.1.Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1764415953&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NDQxNTk1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTE0NmI0MmM3NWEzZDRjNDRlNDE2NjcvYTI0NmM0OGEyZDVkMTNjZWIyMDFjM2U2NjliMGI0MTQ0OTkyNjQ4ZjQyYTFiZWE0YzVmMTU4MzIxMjdjODMxZioifV19&Signature=ltxN%7EoFENPpTcM0z4FQmlEOp5bANZbrJiKEinpY-Tb8xIyn%7EERo1mLwU4gwAd2LOQzn9lrNp6XGWT2pWYNDnR1%7EuFDCNcyoDullTDyv0Fboadum8QNRq9R4sDuzNF5LEKYFVUos-t4Pemy0koqxykE0Dhc0kPXbrwIprT6dL44npXwBUO2fV6cvywo0T5NKvc11Q4T7DOAdIJAMG-uEEiJqzqYkQKQC%7EbIoyYdrM%7EWDs4gsbyHKAxFd6kqqko2gWy%7EOR%7E%7ELpc9pOdQwzQc1064SsJ8I7rF41H5P7SMeXP7ILgw1nyQ079KqfXcoxvtj8P8DZOIVo0pv7vJV25HowpA__&Key-Pair-Id=K2L8F4GPSG1IFC\nResolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.88, 18.238.217.126, 18.238.217.63, ...\nConnecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.88|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4368438944 (4.1G)\nSaving to: â€˜mistral.ggufâ€™\n\nmistral.gguf        100%[===================>]   4.07G   236MB/s    in 14s     \n\n2025-11-29 10:32:47 (305 MB/s) - â€˜mistral.ggufâ€™ saved [4368438944/4368438944]\n\nModel downloaded successfully!\n","output_type":"stream"},{"name":"stderr","text":"llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n","output_type":"stream"},{"name":"stdout","text":"\nMODEL OUTPUT:\n  Hello! How can I assist you today?\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ===========================================\n# 1. Install required packages\n# ===========================================\n!pip install sentence-transformers --quiet\n\n# ===========================================\n# 2. Import libraries\n# ===========================================\nimport pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer, util\nimport re\n\n# ===========================================\n# 3. Load dataset\n# ===========================================\n# Change path if needed\ndf = pd.read_csv(\"/kaggle/input/final-one/telecom_tickets_cleaned1.csv\")\n\n# Show columns\nprint(\"Columns:\", df.columns.tolist())\n\n# ===========================================\n# 4. Pick important columns\n# ===========================================\nCOL_TEXT = \"combined_text\"\nCOL_SOL = \"solutions\"\n\n# ===========================================\n# 5. Clean text function\n# ===========================================\ndef clean_text(t):\n    if pd.isna(t): \n        return \"\"\n    t = str(t).lower()\n    t = re.sub(r'[^a-zA-Z0-9\\s]', ' ', t)\n    t = re.sub(r'\\s+', ' ', t).strip()\n    return t\n\n# Apply cleaning\ndf[COL_TEXT + \"_clean\"] = df[COL_TEXT].apply(clean_text)\ndf[COL_SOL + \"_clean\"] = df[COL_SOL].apply(clean_text)\n\n# ===========================================\n# 6. Load embedding model (free)\n# ===========================================\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")  # small + fast\n\n# ===========================================\n# 7. Build embeddings for the whole dataset\n# ===========================================\ncorpus_embeddings = model.encode(\n    df[COL_TEXT + \"_clean\"].tolist(),\n    convert_to_tensor=True,\n    show_progress_bar=True\n)\n\nprint(\"Embedding shape:\", corpus_embeddings.shape)\n\n# ===========================================\n# 8. Define search function\n# ===========================================\ndef search_solution(query, top_k=1):\n    q = clean_text(query)\n    query_emb = model.encode(q, convert_to_tensor=True)\n\n    # cosine similarity\n    scores = util.cos_sim(query_emb, corpus_embeddings)[0]\n    top_results = scores.topk(top_k)\n\n    result_idx = top_results.indices.cpu().numpy()\n    result_scores = top_results.values.cpu().numpy()\n\n    results = []\n    for idx, score in zip(result_idx, result_scores):\n        results.append({\n            \"score\": float(score),\n            \"ticket_text\": df.iloc[idx][COL_TEXT],\n            \"solution\": df.iloc[idx][COL_SOL]\n        })\n    return results\n\n# ===========================================\n# 9. Example usage\n# ===========================================\nquery = \"Unable to reset password in customer portal\"\nresults = search_solution(query, top_k=3)\n\nfor r in results:\n    print(\"\\n----------------------------------\")\n    print(\"Score:\", r['score'])\n    print(\"Matched Ticket:\\n\", r['ticket_text'])\n    print(\"Solution:\\n\", r['solution'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:53:28.422461Z","iopub.execute_input":"2025-11-29T10:53:28.423990Z","iopub.status.idle":"2025-11-29T10:54:14.001076Z","shell.execute_reply.started":"2025-11-29T10:53:28.423941Z","shell.execute_reply":"2025-11-29T10:54:13.998537Z"}},"outputs":[{"name":"stdout","text":"Columns: ['Ticket ID', 'Ticket Type', 'Product', 'Ticket Description', 'solutions', 'Channel', 'combined_text']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35e182ddbe4a44cc940532bba94943cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"064b7f176e2b4285a6594eacb2cf8159"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db28002e8acb4220b5cbaf327f41da0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb406f7d0fd943158b94824fd3a62c2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02219eb5a4ad47bc9f7b92bc13a16135"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2de0752783ad49538ab495a5eef3d5b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59cb94f31bc5473db5cf851dfcbcdf9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"581fa75dddea4bca84f9400f877887cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f90fc518a54cd9a8ff9368070df9ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec9456d2b0d4bbbb56a8a2e6f317fa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23f2f0827a641f6a05da6298a58463a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/94 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d447b8e1ad8e4f7a95d65aa95b8c61c5"}},"metadata":{}},{"name":"stdout","text":"Embedding shape: torch.Size([3000, 384])\n\n----------------------------------\nScore: 0.43448662757873535\nMatched Ticket:\n router authentication failure | prepaid sim | customer reports router authentication failure while using prepaid sim. issue frequently repeats in different locations/time.\nSolution:\n Fiber/WiFi Resolution: Restart ONT + Router. Check LOS light stability. Reset PPPoE credentials. Push firmware update to router. Schedule technician if line loss observed.\n\n----------------------------------\nScore: 0.43448662757873535\nMatched Ticket:\n router authentication failure | prepaid sim | customer reports router authentication failure while using prepaid sim. issue frequently repeats in different locations/time.\nSolution:\n Fiber/WiFi Resolution: Restart ONT + Router. Check LOS light stability. Reset PPPoE credentials. Push firmware update to router. Schedule technician if line loss observed.\n\n----------------------------------\nScore: 0.43448662757873535\nMatched Ticket:\n router authentication failure | prepaid sim | customer reports router authentication failure while using prepaid sim. issue frequently repeats in different locations/time.\nSolution:\n Fiber/WiFi Resolution: Restart ONT + Router. Check LOS light stability. Reset PPPoE credentials. Push firmware update to router. Schedule technician if line loss observed.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\nquery = \"No signal\"\nresults = search_solution(query, top_k=3)\n\nfor r in results:\n    print(\"\\n----------------------------------\")\n    print(\"Score:\", r['score'])\n    print(\"Matched Ticket:\\n\", r['ticket_text'])\n    print(\"Solution:\\n\", r['solution'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T10:56:44.839714Z","iopub.execute_input":"2025-11-29T10:56:44.840238Z","iopub.status.idle":"2025-11-29T10:56:44.878835Z","shell.execute_reply.started":"2025-11-29T10:56:44.840196Z","shell.execute_reply":"2025-11-29T10:56:44.877574Z"}},"outputs":[{"name":"stdout","text":"\n----------------------------------\nScore: 0.31270545721054077\nMatched Ticket:\n weak signal strength | prepaid sim | customer reports weak signal strength while using prepaid sim. issue frequently repeats in different locations/time.\nSolution:\n Network Troubleshooting: Toggle Airplane mode & reregister to network. Reset mobile network settings + APN. Test SIM in different handset. Check tower coverage + congestion status. Escalate to field team for RF analysis. Offer SLA & callback resolution timeline.\n\n----------------------------------\nScore: 0.31270545721054077\nMatched Ticket:\n weak signal strength | prepaid sim | customer reports weak signal strength while using prepaid sim. issue frequently repeats in different locations/time.\nSolution:\n Network Troubleshooting: Toggle Airplane mode & reregister to network. Reset mobile network settings + APN. Test SIM in different handset. Check tower coverage + congestion status. Escalate to field team for RF analysis. Offer SLA & callback resolution timeline.\n\n----------------------------------\nScore: 0.31270545721054077\nMatched Ticket:\n weak signal strength | prepaid sim | customer reports weak signal strength while using prepaid sim. issue frequently repeats in different locations/time.\nSolution:\n Network Troubleshooting: Toggle Airplane mode & reregister to network. Reset mobile network settings + APN. Test SIM in different handset. Check tower coverage + congestion status. Escalate to field team for RF analysis. Offer SLA & callback resolution timeline.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ==========================================\n# ALTERNATIVE: Simple UI with ipywidgets\n# (If Gradio doesn't work in Kaggle)\n# ==========================================\n\nfrom ipywidgets import widgets, Layout, VBox, HBox, HTML\nfrom IPython.display import display, clear_output\nimport json\n\n# ==========================================\n# Create UI Components\n# ==========================================\n\n# Title\ntitle = HTML(\n    value=\"\"\"\n    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n                padding: 20px; \n                border-radius: 10px; \n                margin-bottom: 20px;'>\n        <h1 style='color: white; margin: 0; text-align: center;'>\n            ğŸ« GenAI Ticket Analysis System\n        </h1>\n        <p style='color: white; margin: 10px 0 0 0; text-align: center;'>\n            Powered by Mistral-7B + Sentence Transformers\n        </p>\n    </div>\n    \"\"\"\n)\n\n# Input text area\nquery_input = widgets.Textarea(\n    value='',\n    placeholder='Enter customer issue here... (e.g., \"Customer reports weak signal strength in multiple locations\")',\n    description='',\n    layout=Layout(width='100%', height='120px'),\n    style={'description_width': 'initial'}\n)\n\n# Analyze button\nanalyze_button = widgets.Button(\n    description='ğŸ” Analyze Ticket',\n    button_style='primary',\n    layout=Layout(width='200px', height='40px'),\n    style={'font_weight': 'bold'}\n)\n\n# Clear button\nclear_button = widgets.Button(\n    description='ğŸ—‘ Clear',\n    button_style='warning',\n    layout=Layout(width='100px', height='40px')\n)\n\n# Output area\noutput_area = widgets.Output(\n    layout=Layout(\n        width='100%', \n        border='2px solid #667eea',\n        border_radius='10px',\n        padding='20px',\n        margin_top='20px'\n    )\n)\n\n# Example buttons\nexample_buttons = []\nexamples = [\n    \"Customer reports no network coverage in multiple locations\",\n    \"Weak signal strength and frequent call drops\",\n    \"Unable to reset password in customer portal\",\n    \"Slow internet speed despite 4G coverage\"\n]\n\nfor example in examples:\n    btn = widgets.Button(\n        description=example[:50] + \"...\",\n        layout=Layout(width='auto', margin='5px'),\n        button_style='info',\n        tooltip=example\n    )\n    example_buttons.append(btn)\n\n# ==========================================\n# Enhanced Analysis Function\n# ==========================================\n\ndef enhanced_analyze(query_text):\n    \"\"\"\n    Analyze ticket with LLM-generated solutions\n    \"\"\"\n    if not query_text or len(query_text.strip()) < 10:\n        return {\n            'error': True,\n            'message': 'Please enter a detailed description (at least 10 characters)'\n        }\n    \n    try:\n        # Step 1: Find similar tickets\n        similar_tickets = search_faiss(query_text, k=5)\n    \n        if not similar_tickets:\n            return {\n                'error': True,\n                'message': 'No similar tickets found'\n            }\n        \n        # Step 2: Generate solutions with Mistral\n        context = \"\\n\\n\".join([\n            f\"Ticket {i+1}: {t['ticket_text'][:150]}\\nSolution: {t['solution'][:150]}\"\n            for i, t in enumerate(similar_tickets[:3])\n        ])\n        \n        prompt = f\"\"\"[INST] Based on this issue: \"{query_text}\"\n\nAnd these similar tickets:\n{context}\n\nProvide 3 ranked solutions with titles, steps, and suitability percentages (must sum to 100%).\nFormat: Solution 1: [title] | Steps: [steps] | Suitability: 50% [/INST]\"\"\"\n        \n        try:\n            llm_response = llm(prompt, max_tokens=600, temperature=0.3)\n            solutions_text = llm_response[\"choices\"][0][\"text\"].strip()\n        except:\n            solutions_text = \"Using similarity-based solutions (LLM unavailable)\"\n        \n        return {\n            'error': False,\n            'query': query_text,\n            'similar_tickets': similar_tickets[:3],\n            'solutions': solutions_text\n        }\n        \n    except Exception as e:\n        return {\n            'error': True,\n            'message': f'Analysis failed: {str(e)}'\n        }\n\n# ==========================================\n# Display Functions\n# ==========================================\n\ndef display_results(result):\n    \"\"\"Display results in formatted HTML\"\"\"\n    with output_area:\n        clear_output()\n        \n        if result['error']:\n            display(HTML(f\"\"\"\n                <div style='background: #fee; padding: 20px; border-radius: 10px; border-left: 5px solid #f00;'>\n                    <h3 style='color: #c00; margin-top: 0;'>âš  Error</h3>\n                    <p>{result['message']}</p>\n                </div>\n            \"\"\"))\n            return\n        \n        # Success - display results\n        html = f\"\"\"\n        <div style='font-family: monospace;'>\n            <div style='background: #e7f3ff; padding: 15px; border-radius: 10px; margin-bottom: 20px;'>\n                <h3 style='margin-top: 0; color: #1976d2;'>ğŸ“ Your Query:</h3>\n                <p style='background: white; padding: 10px; border-radius: 5px;'>{result['query']}</p>\n            </div>\n            \n            <div style='background: #f0f8ff; padding: 15px; border-radius: 10px; margin-bottom: 20px;'>\n                <h3 style='margin-top: 0; color: #2e7d32;'>ğŸ” Top 3 Similar Tickets:</h3>\n        \"\"\"\n        \n        for i, ticket in enumerate(result['similar_tickets'], 1):\n            html += f\"\"\"\n                <div style='background: white; padding: 15px; border-radius: 5px; margin-bottom: 10px; border-left: 4px solid #4caf50;'>\n                    <strong>Match #{i} - Score: {ticket['score']*100:.1f}%</strong><br><br>\n                    <strong>Issue:</strong><br>\n                    <p style='margin: 5px 0; padding-left: 10px;'>{ticket['ticket_text'][:200]}...</p>\n                    <strong>Solution:</strong><br>\n                    <p style='margin: 5px 0; padding-left: 10px; color: #2e7d32;'>{ticket['solution'][:200]}...</p>\n                </div>\n            \"\"\"\n        \n        html += f\"\"\"\n            </div>\n            \n            <div style='background: #fff3e0; padding: 15px; border-radius: 10px;'>\n                <h3 style='margin-top: 0; color: #f57c00;'>ğŸ¤– AI-Generated Solutions:</h3>\n                <div style='background: white; padding: 15px; border-radius: 5px; white-space: pre-wrap; line-height: 1.6;'>\n{result['solutions']}\n                </div>\n            </div>\n            \n            <div style='margin-top: 20px; padding: 15px; background: #e8f5e9; border-radius: 10px; text-align: center;'>\n                <strong style='color: #2e7d32;'>âœ… Analysis Complete!</strong>\n            </div>\n        </div>\n        \"\"\"\n        \n        display(HTML(html))\n\n# ==========================================\n# Button Click Handlers\n# ==========================================\n\ndef on_analyze_clicked(b):\n    \"\"\"Handle analyze button click\"\"\"\n    query = query_input.value\n    \n    with output_area:\n        clear_output()\n        display(HTML(\"\"\"\n            <div style='text-align: center; padding: 30px;'>\n                <h3>ğŸ”„ Analyzing ticket...</h3>\n                <p>Please wait while we search for similar tickets and generate solutions...</p>\n            </div>\n        \"\"\"))\n    \n    # Perform analysis\n    result = enhanced_analyze(query)\n    \n    # Display results\n    display_results(result)\n\ndef on_clear_clicked(b):\n    \"\"\"Handle clear button click\"\"\"\n    query_input.value = ''\n    with output_area:\n        clear_output()\n\ndef on_example_clicked(btn):\n    \"\"\"Handle example button click\"\"\"\n    query_input.value = btn.tooltip\n\n# ==========================================\n# Attach Event Handlers\n# ==========================================\n\nanalyze_button.on_click(on_analyze_clicked)\nclear_button.on_click(on_clear_clicked)\n\nfor btn in example_buttons:\n    btn.on_click(on_example_clicked)\n\n# ==========================================\n# Layout and Display\n# ==========================================\n\n# Instructions\ninstructions = HTML(\n    value=\"\"\"\n    <div style='background: #f5f5f5; padding: 15px; border-radius: 10px; margin-bottom: 20px;'>\n        <h3 style='margin-top: 0;'>ğŸ“– How to Use:</h3>\n        <ol style='margin: 10px 0;'>\n            <li>Enter a customer issue in the text box below</li>\n            <li>Click \"Analyze Ticket\" to get AI-powered solutions</li>\n            <li>Or click any example button to try pre-made queries</li>\n        </ol>\n        <p style='margin: 10px 0 0 0;'><strong>Note:</strong> Analysis takes 5-10 seconds</p>\n    </div>\n    \"\"\"\n)\n\n# Example section\nexample_section = HTML(\n    value=\"<h4>ğŸ’¡ Try These Examples:</h4>\"\n)\n\n# Assemble UI\nui = VBox([\n    title,\n    instructions,\n    HTML(\"<h4>ğŸ“ Enter Ticket Description:</h4>\"),\n    query_input,\n    HBox([analyze_button, clear_button], layout=Layout(margin='10px 0')),\n    example_section,\n    HBox(example_buttons[:2], layout=Layout(margin='5px 0')),\n    HBox(example_buttons[2:], layout=Layout(margin='5px 0')),\n    output_area\n], layout=Layout(padding='20px'))\n\n# ==========================================\n# Display the UI\n# ==========================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ¯ TICKET ANALYSIS UI READY!\")\nprint(\"=\"*80)\nprint(f\"\\nâœ“ {len(df)} tickets loaded\")\nprint(\"âœ“ Embedding model ready\")\nprint(\"âœ“ Mistral LLM loaded\")\nprint(\"\\nğŸ‘‡ Use the interface below:\\n\")\n\ndisplay(ui)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… UI is active! Enter a query and click 'Analyze Ticket'\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T11:25:37.401256Z","iopub.execute_input":"2025-11-29T11:25:37.401682Z","iopub.status.idle":"2025-11-29T11:25:37.468701Z","shell.execute_reply.started":"2025-11-29T11:25:37.401651Z","shell.execute_reply":"2025-11-29T11:25:37.467415Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nğŸ¯ TICKET ANALYSIS UI READY!\n================================================================================\n\nâœ“ 3000 tickets loaded\nâœ“ Embedding model ready\nâœ“ Mistral LLM loaded\n\nğŸ‘‡ Use the interface below:\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value=\"\\n    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \\nâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ed4092fa2954122957ce8c6592c14d8"}},"metadata":{}},{"name":"stdout","text":"\n================================================================================\nâœ… UI is active! Enter a query and click 'Analyze Ticket'\n================================================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ================================\n# Install libraries\n# ================================\n!pip install -q sentence-transformers faiss-cpu chromadb llama-cpp-python\n\n# ================================\n# Imports\n# ================================\nimport pandas as pd\nimport numpy as np\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nfrom llama_cpp import Llama\nimport chromadb\nfrom chromadb.config import Settings\n\n# ================================\n# Load dataset\n# ================================\ndf = pd.read_csv(\"/kaggle/input/final-one/telecom_tickets_cleaned1.csv\")\nprint(\"Columns:\", df.columns)\n\n# ================================\n# Clean text\n# ================================\ndef clean(x):\n    if pd.isna(x):\n        return \"\"\n    return (\n        str(x)\n        .replace(\"\\n\", \" \")\n        .replace(\"\\r\", \" \")\n        .strip()\n    )\n\ndf[\"combined_text\"] = df[\"combined_text\"].astype(str).apply(clean)\ndf[\"solutions\"] = df[\"solutions\"].astype(str).apply(clean)\n\n# ================================\n# Embedding model\n# ================================\nprint(\"Loading embedding model...\")\nembed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\ntexts = df[\"combined_text\"].tolist()\n\nprint(\"Generating embeddings...\")\nembeddings = embed_model.encode(texts, batch_size=32, show_progress_bar=True)\nembeddings = np.array(embeddings).astype(\"float32\")\n\nprint(\"Embedding shape:\", embeddings.shape)\n\n# ================================\n# Build FAISS index\n# ================================\ndim = embeddings.shape[1]\nfaiss_index = faiss.IndexFlatL2(dim)\nfaiss_index.add(embeddings)\nprint(\"FAISS index vectors:\", faiss_index.ntotal)\n\n# ================================\n# FAISS search function\n# ================================\ndef search_faiss(query, k=3):\n    q_embed = embed_model.encode([query]).astype(\"float32\")\n    distances, indices = faiss_index.search(q_embed, k)\n\n    results = []\n    for idx, score in zip(indices[0], distances[0]):\n        row = df.iloc[idx]\n        results.append({\n            \"score\": float(score),\n            \"ticket\": row[\"combined_text\"],\n            \"solution\": row[\"solutions\"]\n        })\n    return results\n\n# ================================\n# Download Mistral GGUF model\n# ================================\n!wget -q -O mistral.gguf https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf\nprint(\"Model downloaded.\")\n\n# ================================\n# Load Llama model\n# ================================\nprint(\"Loading Llama model (this will take ~20 seconds)...\")\nllm = Llama(\n    model_path=\"mistral.gguf\",\n    n_ctx=8192,\n    n_threads=4,\n    temperature=0.3,\n    max_tokens=1024\n)\n\n# ================================\n# RAG generation function\n# ================================\ndef generate_solutions(user_query, retrieved):\n    context = \"\"\n    for item in retrieved:\n        context += f\"\"\"\nPast Ticket:\n{item['ticket']}\n\nResolved Solution:\n{item['solution']}\n-------------------------\n\"\"\"\n\n    prompt = f\"\"\"\nYou are a telecom expert support system using RAG.\n\nUSER ISSUE:\n{user_query}\n\nREFERENCE PAST TICKETS:\n{context}\n\nTASK:\n1. Generate 3 recommended solutions.\n2. Assign a suitability percentage to each.\n3. Output ONLY in this exact format:\n\nRecommended Solutions:\n1. <solution> (<percentage>%)\n2. <solution> (<percentage>%)\n3. <solution> (<percentage>%)\n\nBegin now:\n\"\"\"\n\n    out = llm(prompt)\n    return out[\"choices\"][0][\"text\"]\n\n# ================================\n# RUN FULL PIPELINE\n# ================================\nquery = \"internet disconnects frequently and router keeps restarting\"\n\nprint(\"\\nSearching FAISS...\")\nretrieved = search_faiss(query, k=3)\n\nprint(\"\\nRetrieved Results:\")\nfor r in retrieved:\n    print(\"\\nScore:\", r[\"score\"])\n    print(\"Ticket:\", r[\"ticket\"])\n    print(\"Solution:\", r[\"solution\"])\n\nprint(\"\\n========== FINAL AI-GENERATED SOLUTION ==========\\n\")\nfinal_output = generate_solutions(query, retrieved)\nprint(final_output)\n","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-11-29T11:57:53.171390Z","iopub.execute_input":"2025-11-29T11:57:53.171631Z","iopub.status.idle":"2025-11-29T12:09:04.341887Z","shell.execute_reply.started":"2025-11-29T11:57:53.171613Z","shell.execute_reply":"2025-11-29T12:09:04.341122Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.6/456.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.1/456.1 kB\u001b[0m \u001b[31m741.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m39.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m80.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\ngoogle-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-29 12:01:39.656458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764417699.826651      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764417699.877409      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Columns: Index(['Ticket ID', 'Ticket Type', 'Product', 'Ticket Description',\n       'solutions', 'Channel', 'combined_text'],\n      dtype='object')\nLoading embedding model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a0a09e831f42ef874e86e4d51e8e19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb62f143032c47049e383b1501d26904"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6257e534c94b7e8b74140a9939f7b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be210e449cb942c08200518e52fe1112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"370f7a01050b4f5a9d5705908095ec9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6312f8b290604bcd9f6e164ba43a401d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"003b4657be6449a6bfe270d6183bd067"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ead1658249d4bbc89b8855807cc58db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d16c95112c8b4ed2a70d0c7d0f1000d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"625cae2caee4453ea1f6a5c1e880f5be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5324fd23932a4bc3af57c0d9effc1e23"}},"metadata":{}},{"name":"stdout","text":"Generating embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/94 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"337de22a4cde48bdbefcb928a3db067d"}},"metadata":{}},{"name":"stdout","text":"Embedding shape: (3000, 384)\nFAISS index vectors: 3000\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nllama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from mistral.gguf (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 15\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","output_type":"stream"},{"name":"stdout","text":"Model downloaded.\nLoading Llama model (this will take ~20 seconds)...\n","output_type":"stream"},{"name":"stderr","text":"llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  19:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nprint_info: file format = GGUF V2\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 4.07 GiB (4.83 BPW) \ninit_tokenizer: initializing tokenizer for type 1\nload: control token:      2 '</s>' is not marked as EOG\nload: control token:      1 '<s>' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: printing all EOG tokens:\nload:   - 2 ('</s>')\nload: special tokens cache size = 3\nload: token to piece cache size = 0.1637 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 32768\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 32\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 14336\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 32768\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 7B\nprint_info: model params     = 7.24 B\nprint_info: general.name     = mistralai_mistral-7b-instruct-v0.1\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32000\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'\nprint_info: EOS token        = 2 '</s>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 2 '</s>'\nprint_info: max token length = 48\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CPU, is_swa = 0\nload_tensors: layer   1 assigned to device CPU, is_swa = 0\nload_tensors: layer   2 assigned to device CPU, is_swa = 0\nload_tensors: layer   3 assigned to device CPU, is_swa = 0\nload_tensors: layer   4 assigned to device CPU, is_swa = 0\nload_tensors: layer   5 assigned to device CPU, is_swa = 0\nload_tensors: layer   6 assigned to device CPU, is_swa = 0\nload_tensors: layer   7 assigned to device CPU, is_swa = 0\nload_tensors: layer   8 assigned to device CPU, is_swa = 0\nload_tensors: layer   9 assigned to device CPU, is_swa = 0\nload_tensors: layer  10 assigned to device CPU, is_swa = 0\nload_tensors: layer  11 assigned to device CPU, is_swa = 0\nload_tensors: layer  12 assigned to device CPU, is_swa = 0\nload_tensors: layer  13 assigned to device CPU, is_swa = 0\nload_tensors: layer  14 assigned to device CPU, is_swa = 0\nload_tensors: layer  15 assigned to device CPU, is_swa = 0\nload_tensors: layer  16 assigned to device CPU, is_swa = 0\nload_tensors: layer  17 assigned to device CPU, is_swa = 0\nload_tensors: layer  18 assigned to device CPU, is_swa = 0\nload_tensors: layer  19 assigned to device CPU, is_swa = 0\nload_tensors: layer  20 assigned to device CPU, is_swa = 0\nload_tensors: layer  21 assigned to device CPU, is_swa = 0\nload_tensors: layer  22 assigned to device CPU, is_swa = 0\nload_tensors: layer  23 assigned to device CPU, is_swa = 0\nload_tensors: layer  24 assigned to device CPU, is_swa = 0\nload_tensors: layer  25 assigned to device CPU, is_swa = 0\nload_tensors: layer  26 assigned to device CPU, is_swa = 0\nload_tensors: layer  27 assigned to device CPU, is_swa = 0\nload_tensors: layer  28 assigned to device CPU, is_swa = 0\nload_tensors: layer  29 assigned to device CPU, is_swa = 0\nload_tensors: layer  30 assigned to device CPU, is_swa = 0\nload_tensors: layer  31 assigned to device CPU, is_swa = 0\nload_tensors: layer  32 assigned to device CPU, is_swa = 0\nload_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\nload_tensors:   CPU_REPACK model buffer size =  3204.00 MiB\nload_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\nrepack: repack tensor blk.0.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.0.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.0.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.1.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.2.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.3.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.4.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.4.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.5.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.5.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.6.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.7.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.7.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.8.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.8.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.8.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.9.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.9.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.10.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.10.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.10.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.11.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.11.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.11.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.12.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.13.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.13.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.14.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.14.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.15.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.15.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.16.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.16.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.17.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.17.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.17.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.18.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.18.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.19.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.19.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.19.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.20.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.20.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\nrepack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.21.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.22.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n.repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.23.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.23.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n.repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\nrepack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.24.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.24.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.25.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.25.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.25.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n.repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\nrepack: repack tensor blk.26.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.26.attn_v.weight with q4_K_8x8\nrepack: repack tensor blk.26.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n.repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.27.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.28.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.29.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.30.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\nrepack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n.repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\nrepack: repack tensor blk.31.attn_output.weight with q4_K_8x8\nrepack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n.repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n...................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 8192\nllama_context: n_ctx_per_seq = 8192\nllama_context: n_batch       = 512\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: kv_unified    = false\nllama_context: freq_base     = 10000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nset_abort_callback: call\nllama_context:        CPU  output buffer size =     0.12 MiB\ncreate_memory: n_ctx = 8192 (padded)\nllama_kv_cache_unified: layer   0: dev = CPU\nllama_kv_cache_unified: layer   1: dev = CPU\nllama_kv_cache_unified: layer   2: dev = CPU\nllama_kv_cache_unified: layer   3: dev = CPU\nllama_kv_cache_unified: layer   4: dev = CPU\nllama_kv_cache_unified: layer   5: dev = CPU\nllama_kv_cache_unified: layer   6: dev = CPU\nllama_kv_cache_unified: layer   7: dev = CPU\nllama_kv_cache_unified: layer   8: dev = CPU\nllama_kv_cache_unified: layer   9: dev = CPU\nllama_kv_cache_unified: layer  10: dev = CPU\nllama_kv_cache_unified: layer  11: dev = CPU\nllama_kv_cache_unified: layer  12: dev = CPU\nllama_kv_cache_unified: layer  13: dev = CPU\nllama_kv_cache_unified: layer  14: dev = CPU\nllama_kv_cache_unified: layer  15: dev = CPU\nllama_kv_cache_unified: layer  16: dev = CPU\nllama_kv_cache_unified: layer  17: dev = CPU\nllama_kv_cache_unified: layer  18: dev = CPU\nllama_kv_cache_unified: layer  19: dev = CPU\nllama_kv_cache_unified: layer  20: dev = CPU\nllama_kv_cache_unified: layer  21: dev = CPU\nllama_kv_cache_unified: layer  22: dev = CPU\nllama_kv_cache_unified: layer  23: dev = CPU\nllama_kv_cache_unified: layer  24: dev = CPU\nllama_kv_cache_unified: layer  25: dev = CPU\nllama_kv_cache_unified: layer  26: dev = CPU\nllama_kv_cache_unified: layer  27: dev = CPU\nllama_kv_cache_unified: layer  28: dev = CPU\nllama_kv_cache_unified: layer  29: dev = CPU\nllama_kv_cache_unified: layer  30: dev = CPU\nllama_kv_cache_unified: layer  31: dev = CPU\nllama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB\nllama_kv_cache_unified: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 1\nllama_context: max_nodes = 2328\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\ngraph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\nllama_context:        CPU compute buffer size =   564.01 MiB\nllama_context: graph nodes  = 1126\nllama_context: graph splits = 1\nCPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \nModel metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nUsing fallback chat format: llama-2\n","output_type":"stream"},{"name":"stdout","text":"\nSearching FAISS...\n\nRetrieved Results:\n\nScore: 0.6906498670578003\nTicket: frequent call drops | router + static ip | customer reports frequent call drops while using router + static ip. issue frequently repeats in different locations/time.\nSolution: Network Troubleshooting: Toggle Airplane mode & reregister to network. Reset mobile network settings + APN. Test SIM in different handset. Check tower coverage + congestion status. Escalate to field team for RF analysis. Offer SLA & callback resolution timeline.\n\nScore: 0.6906498670578003\nTicket: frequent call drops | router + static ip | customer reports frequent call drops while using router + static ip. issue frequently repeats in different locations/time.\nSolution: Network Troubleshooting: Toggle Airplane mode & reregister to network. Reset mobile network settings + APN. Test SIM in different handset. Check tower coverage + congestion status. Escalate to field team for RF analysis. Offer SLA & callback resolution timeline.\n\nScore: 0.731448769569397\nTicket: signal fluctuation | router + static ip | customer reports signal fluctuation while using router + static ip. issue frequently repeats in different locations/time.\nSolution: Network Troubleshooting: Toggle Airplane mode & reregister to network. Reset mobile network settings + APN. Test SIM in different handset. Check tower coverage + congestion status. Escalate to field team for RF analysis. Offer SLA & callback resolution timeline.\n\n========== FINAL AI-GENERATED SOLUTION ==========\n\n","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   40827.28 ms\nllama_perf_context_print: prompt eval time =   40826.63 ms /   482 tokens (   84.70 ms per token,    11.81 tokens per second)\nllama_perf_context_print:        eval time =    3594.38 ms /    15 runs   (  239.63 ms per token,     4.17 tokens per second)\nllama_perf_context_print:       total time =   44429.60 ms /   497 tokens\nllama_perf_context_print:    graphs reused =         14\n","output_type":"stream"},{"name":"stdout","text":"\nSolution 1:\nCheck router settings and ensure they are optimized\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nquery = \"internet disconnects frequently and router keeps restarting\"\n\nprint(\"\\nSearching FAISS...\")\nretrieved = search_faiss(query, k=3)\n\nprint(\"\\nRetrieved Results:\")\nfor r in retrieved:\n    print(\"\\nScore:\", r[\"score\"])\n    print(\"Ticket:\", r[\"ticket\"])\n    print(\"Solution:\", r[\"solution\"])\n\nprint(\"\\n========== FINAL AI-GENERATED SOLUTION ==========\\n\")\nfinal_output = generate_solutions(query, retrieved)\nprint(final_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:09:04.343042Z","iopub.execute_input":"2025-11-29T12:09:04.343281Z","iopub.status.idle":"2025-11-29T12:09:08.196708Z","shell.execute_reply.started":"2025-11-29T12:09:04.343257Z","shell.execute_reply":"2025-11-29T12:09:08.195555Z"}},"outputs":[{"name":"stderr","text":"Llama.generate: 481 prefix-match hit, remaining 1 prompt tokens to eval\n","output_type":"stream"},{"name":"stdout","text":"\nSearching FAISS...\n\nRetrieved Results:\n\nScore: 0.6906498670578003\nTicket: frequent call drops | router + static ip | customer reports frequent call drops while using router + static ip. issue frequently repeats in different locations/time.\nSolution: Network Troubleshooting: Toggle Airplane mode & reregister to network. Reset mobile network settings + APN. Test SIM in different handset. Check tower coverage + congestion status. Escalate to field team for RF analysis. Offer SLA & callback resolution timeline.\n\nScore: 0.6906498670578003\nTicket: frequent call drops | router + static ip | customer reports frequent call drops while using router + static ip. issue frequently repeats in different locations/time.\nSolution: Network Troubleshooting: Toggle Airplane mode & reregister to network. Reset mobile network settings + APN. Test SIM in different handset. Check tower coverage + congestion status. Escalate to field team for RF analysis. Offer SLA & callback resolution timeline.\n\nScore: 0.731448769569397\nTicket: signal fluctuation | router + static ip | customer reports signal fluctuation while using router + static ip. issue frequently repeats in different locations/time.\nSolution: Network Troubleshooting: Toggle Airplane mode & reregister to network. Reset mobile network settings + APN. Test SIM in different handset. Check tower coverage + congestion status. Escalate to field team for RF analysis. Offer SLA & callback resolution timeline.\n\n========== FINAL AI-GENERATED SOLUTION ==========\n\n","output_type":"stream"},{"name":"stderr","text":"llama_perf_context_print:        load time =   40827.28 ms\nllama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\nllama_perf_context_print:        eval time =    3820.06 ms /    16 runs   (  238.75 ms per token,     4.19 tokens per second)\nllama_perf_context_print:       total time =    3828.95 ms /    17 tokens\nllama_perf_context_print:    graphs reused =         16\n","output_type":"stream"},{"name":"stdout","text":"\n\n\nRecommended Solutions:\n1. Toggle Airplane mode\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ================================\n# Install Gradio\n# ================================\n!pip install -q gradio\n\n# ================================\n# Add this to your existing Kaggle code after all your setup\n# ================================\nimport gradio as gr\n\ndef analyze_ticket(ticket_description):\n    \"\"\"\n    Main function that Gradio will call\n    \"\"\"\n    if not ticket_description.strip():\n        return \"âš  Please enter a ticket description\", \"\", \"\"\n    \n    try:\n        # Search FAISS for similar tickets\n        retrieved = search_faiss(ticket_description, k=3)\n        \n        # Generate solutions using LLM\n        final_output = generate_solutions(ticket_description, retrieved)\n        \n        # Format retrieved tickets for display\n        retrieved_html = \"<div style='background: #f8f9fa; padding: 20px; border-radius: 10px;'>\"\n        retrieved_html += \"<h3 style='color: #2563eb; margin-bottom: 15px;'>ğŸ“‹ Similar Resolved Tickets</h3>\"\n        \n        for idx, item in enumerate(retrieved, 1):\n            match_score = 1 - item['score']  # Convert distance to similarity\n            retrieved_html += f\"\"\"\n            <div style='background: white; padding: 15px; margin-bottom: 15px; border-radius: 8px; border-left: 4px solid #10b981;'>\n                <div style='font-size: 12px; color: #6b7280; margin-bottom: 8px;'>\n                    Match Score: {match_score:.2f}\n                </div>\n                <div style='margin-bottom: 10px;'>\n                    <strong style='color: #374151;'>Past Ticket:</strong>\n                    <p style='margin: 5px 0; color: #4b5563;'>{item['ticket']}</p>\n                </div>\n                <div>\n                    <strong style='color: #059669;'>Resolution:</strong>\n                    <p style='margin: 5px 0; color: #4b5563;'>{item['solution']}</p>\n                </div>\n            </div>\n            \"\"\"\n        retrieved_html += \"</div>\"\n        \n        # Format AI recommendations\n        recommendations_html = \"<div style='background: #f0f9ff; padding: 20px; border-radius: 10px;'>\"\n        recommendations_html += \"<h3 style='color: #2563eb; margin-bottom: 15px;'>ğŸ¤– AI-Generated Solutions</h3>\"\n        recommendations_html += f\"<pre style='background: white; padding: 15px; border-radius: 8px; white-space: pre-wrap; font-family: system-ui;'>{final_output}</pre>\"\n        recommendations_html += \"</div>\"\n        \n        return recommendations_html, retrieved_html, \"âœ… Analysis Complete!\"\n        \n    except Exception as e:\n        error_msg = f\"âŒ Error: {str(e)}\"\n        return error_msg, \"\", error_msg\n\n# ================================\n# Create Gradio Interface\n# ================================\nwith gr.Blocks(title=\"GenAI Ticket Analysis Assistant\", theme=gr.themes.Soft()) as demo:\n    \n    gr.Markdown(\"\"\"\n    # ğŸ¯ GenAI-Powered Ticket Analysis for Telecom\n    ### AI-powered solution recommendations based on similar resolved tickets\n    Enter a ticket description below to get instant recommendations.\n    \"\"\")\n    \n    with gr.Row():\n        with gr.Column():\n            ticket_input = gr.Textbox(\n                label=\"Ticket Description\",\n                placeholder=\"e.g., Customer reports frequent internet disconnections and router keeps restarting...\",\n                lines=6,\n                info=\"Describe the customer's issue in detail\"\n            )\n            \n            analyze_btn = gr.Button(\"ğŸ” Analyze Ticket\", variant=\"primary\", size=\"lg\")\n            status = gr.Textbox(label=\"Status\", interactive=False, show_label=False)\n    \n    with gr.Row():\n        with gr.Column():\n            recommendations_output = gr.HTML(label=\"AI Recommendations\")\n        \n        with gr.Column():\n            retrieved_output = gr.HTML(label=\"Similar Past Tickets\")\n    \n    # Example tickets\n    gr.Markdown(\"### ğŸ“ Example Tickets (Click to try)\")\n    gr.Examples(\n        examples=[\n            [\"Internet disconnects frequently and router keeps restarting\"],\n            [\"Customer cannot access email, getting authentication errors repeatedly\"],\n            [\"Mobile data extremely slow in residential area, calls dropping\"],\n            [\"Billing shows incorrect charges for international calls not made\"],\n            [\"New SIM card not activating after 24 hours, no service\"]\n        ],\n        inputs=ticket_input\n    )\n    \n    # Button click event\n    analyze_btn.click(\n        fn=analyze_ticket,\n        inputs=ticket_input,\n        outputs=[recommendations_output, retrieved_output, status]\n    )\n\n# ================================\n# Launch the interface\n# ================================\ndemo.launch(\n    share=True,  # Creates a public link you can share\n    debug=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T12:09:13.482882Z","iopub.execute_input":"2025-11-29T12:09:13.483509Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://3369fb80366f11a7c0.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://3369fb80366f11a7c0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stderr","text":"Llama.generate: 23 prefix-match hit, remaining 459 prompt tokens to eval\nllama_perf_context_print:        load time =   40827.28 ms\nllama_perf_context_print: prompt eval time =   40436.47 ms /   459 tokens (   88.10 ms per token,    11.35 tokens per second)\nllama_perf_context_print:        eval time =    3930.41 ms /    15 runs   (  262.03 ms per token,     3.82 tokens per second)\nllama_perf_context_print:       total time =   44375.77 ms /   474 tokens\nllama_perf_context_print:    graphs reused =         14\nLlama.generate: 23 prefix-match hit, remaining 381 prompt tokens to eval\nllama_perf_context_print:        load time =   40827.28 ms\nllama_perf_context_print: prompt eval time =   32466.21 ms /   381 tokens (   85.21 ms per token,    11.74 tokens per second)\nllama_perf_context_print:        eval time =    3822.55 ms /    15 runs   (  254.84 ms per token,     3.92 tokens per second)\nllama_perf_context_print:       total time =   36296.04 ms /   396 tokens\nllama_perf_context_print:    graphs reused =         13\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}